# CodeQL Configuration

# Path to CodeQL executable (required)
# Examples:
#   Linux/macOS: /usr/local/bin/codeql or /path/to/codeql
#   Windows: IMPORTANT - Path MUST end with .cmd
#            Example: C:\path\to\codeql\codeql.cmd
#            Use forward slashes or escaped backslashes: C:/path/to/codeql/codeql.cmd
#            Or use raw string format: r"C:\path\to\codeql\codeql.cmd"
CODEQL_PATH="your_codeql_path"

# GitHub Configuration (optional, for higher rate limits)
# Get token from: https://github.com/settings/tokens
# GITHUB_TOKEN=ghp_your_token_here

# LLM Configuration
# Copy this file to .env and fill in your API keys

# Provider selection (required)
# Allowed providers: openai, azure, gemini

# Model name (required, provider-specific)
# Examples by provider:
#   OpenAI: gpt-4o, gpt-4o-mini, gpt-4-turbo
#   Azure: gpt-4o, gpt-4
#   Google AI Studio: gemini-2.5-flash, gemini-2.0-flash

# Optional: Override default LLM parameters
# NOTE:
# Do NOT increase these values unless you fully understand the impact.
# Lower values keep the model stable and deterministic â€” critical for security analysis.
# Higher values may cause the model to become inconsistent, creative, or hallucinate results.
# Recommended: leave these at their default values.
# LLM_TEMPERATURE=0.2
# LLM_TOP_P=0.2

# ============================================================================
# Provider-Specific Configuration
# ============================================================================
# Uncomment and fill in the section for your chosen provider

# ----------------------------------------------------------------------------
# OpenAI
# ----------------------------------------------------------------------------
PROVIDER=openai
MODEL=gpt-4o
OPENAI_API_KEY="your_api_key"

# ----------------------------------------------------------------------------
# Azure OpenAI
# ----------------------------------------------------------------------------
# AZURE_OPENAI_API_KEY="your_api_key"
# AZURE_OPENAI_ENDPOINT="https://your-name.openai.azure.com/"
# AZURE_OPENAI_API_VERSION="2024-08-01-preview"
# PROVIDER=azure
# MODEL=gpt-4o

# ----------------------------------------------------------------------------
# Google AI Studio
# ----------------------------------------------------------------------------
# GOOGLE_API_KEY="your_api_key"
# PROVIDER=gemini
# MODEL=gemini-2.5-flash

# Logging Configuration

# DEBUG, INFO, WARNING, ERROR
LOG_LEVEL=INFO

# Optional: path to log file (e.g., logs/vulnhalla.log)
# If empty or commented out, no file logging is used
# LOG_FILE=logs/vulnhalla.log
LOG_FILE=

# default or json
LOG_FORMAT=default

# Console format control:
# - Default: INFO messages are minimal (message only) 
#           WARNING/ERROR/CRITICAL use simple format (LEVEL - message)
# - If LOG_VERBOSE_CONSOLE=true: 
#   WARNING/ERROR/CRITICAL use full format
#   (timestamp - logger - level - message)
# - INFO always remains minimal regardless of verbose mode
LOG_VERBOSE_CONSOLE=false

# Control third-party library logging verbosity (LiteLLM, urllib3, requests). Default: ERROR
THIRD_PARTY_LOG_LEVEL=ERROR